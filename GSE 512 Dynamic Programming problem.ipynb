{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GSE 512 Macroeconomic Analysis Final Exam\n",
    "### Michael Chan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### I deleted problem 1 since I don't want to publish any comprehensive exam style questions. Problem 2 is a dynamic programming problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Include at least four states.  Let one of the states be that the student gets stuck in jail. (5 points)\n",
    "2. Let the control set include: watch TV; work out; study; go to the bar; and others as you wish. (5 points)\n",
    "  \n",
    "  \n",
    "These were generated using the interactive code from 11 February 2021 notebook. Left that out to reduce clutter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_names = ['In Jail', 'Feel Sick', 'Feel Normal', 'Feel Great']\n",
    "control_names = ['Watch TV','Work Out','Study','go to the bar','rob a bank']\n",
    "reward = np.array([[-50,-70,-60,-100,200],[-5,-15,-2,-30,10],[5,5,4,30,100],[10,15,12,20,120]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#some paramaters to work with\n",
    "beta=0.97"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[0.9, 0. , 0.1, 0. ],\n",
       "        [0. , 0.6, 0.4, 0. ],\n",
       "        [0. , 0.1, 0.7, 0.2],\n",
       "        [0. , 0.1, 0.7, 0.2]]),\n",
       " array([[0.85, 0.  , 0.15, 0.  ],\n",
       "        [0.05, 0.55, 0.4 , 0.  ],\n",
       "        [0.1 , 0.2 , 0.6 , 0.1 ],\n",
       "        [0.2 , 0.1 , 0.5 , 0.2 ]]),\n",
       " array([[0.85, 0.  , 0.15, 0.  ],\n",
       "        [0.  , 0.4 , 0.6 , 0.  ],\n",
       "        [0.  , 0.2 , 0.7 , 0.1 ],\n",
       "        [0.  , 0.1 , 0.8 , 0.1 ]]),\n",
       " array([[1.  , 0.  , 0.  , 0.  ],\n",
       "        [0.2 , 0.7 , 0.1 , 0.  ],\n",
       "        [0.15, 0.35, 0.5 , 0.  ],\n",
       "        [0.1 , 0.2 , 0.6 , 0.1 ]]),\n",
       " array([[1.  , 0.  , 0.  , 0.  ],\n",
       "        [0.7 , 0.1 , 0.2 , 0.  ],\n",
       "        [0.6 , 0.1 , 0.  , 0.3 ],\n",
       "        [0.45, 0.15, 0.2 , 0.2 ]])]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transition #made with the interactive code to create transition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bellman(J, R, M, beta):\n",
    "    \"\"\"\n",
    "    J current value function,\n",
    "    R reward function,\n",
    "    M transition rule, \n",
    "    beta discount factor\n",
    "    \"\"\"\n",
    "    n = R.shape[0]  #number of states\n",
    "    k = R.shape[1]  #how many controls\n",
    "    if len(M) != k :\n",
    "        print(\"There is not the proper number of transition matrices.\")\n",
    "        return\n",
    "    for i in range(k):\n",
    "        if M[i].shape != (n,n) :\n",
    "            print('Markov matrix', i, 'has the wrong size.')\n",
    "            return\n",
    "        if not np.allclose(sum(M[i].T),1):\n",
    "            print('Markov matrix', i, 'is incorrect.')\n",
    "            \n",
    "    Next = np.empty_like(R) # create an empty n x k matrix\n",
    "    \n",
    "    for x in range(n):\n",
    "        for y in range(k):\n",
    "            Next[x,y] = np.dot(M[y][x],J)  #the expected value of next period's value function\n",
    "                                            #given we're in state x and chose control y   \n",
    "    \n",
    "    next_J = np.empty_like(J)  #value function for the next iteration\n",
    "    \n",
    "    for x in range(n):\n",
    "        next_J[x] = np.max(R[x, :] + beta*Next[x,:])  #the heart of the Bellman recursion\n",
    "        # the maximum is taken over all the columns, for a fixed state x\n",
    "        # each column is a control, of course\n",
    "    return next_J"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_value(R,M,beta,show):  #R is reward function, M transition rule, beta is discount factor\n",
    "    \"\"\"\n",
    "    R is the reward\n",
    "    M is the transition rule\n",
    "    beta is the discount factor\n",
    "    show is a logical variable toggles between printing the iterations\n",
    "    \"\"\"\n",
    "    n = R.shape[0]  # number of states\n",
    "    J = np.zeros(n)      # Initial guess\n",
    "    next_J = np.empty(n)  # Stores updated guess\n",
    "    max_iter = 500  #in case you are stuck in an infinite loop\n",
    "    i = 0   # my counter for iterations\n",
    "\n",
    "    while i < max_iter:\n",
    "        next_J = bellman(J, R, M, beta)  # give it J and spitout next iterate of J\n",
    "        if show == True:\n",
    "            print(next_J)\n",
    "        if np.allclose(next_J, J):  #check if the value functions are squeezing together\n",
    "            break\n",
    "        else:\n",
    "            J[:] = next_J   # Copy contents of next_J to J\n",
    "            i += 1\n",
    "\n",
    "    return(J)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "J = compute_value(reward,transition,beta,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimal_policy(J, R, M, beta, state_names, control_names):\n",
    "    \"\"\"\n",
    "    J is the true value function,\n",
    "    R is reward function, \n",
    "    M transition rule\n",
    "    beta is discount factor\n",
    "    state_names is a list of the states\n",
    "    control_names is a list of the controls\n",
    "    \"\"\"\n",
    "    n = R.shape[0]  # number of states\n",
    "    k = R.shape[1]   #number of controls\n",
    "    Next = np.empty_like(R)  #create an empty array\n",
    "    for x in range(n):\n",
    "        for y in range(k):\n",
    "            Next[x,y] = np.dot(M[y][x],J) #compute the true Bellman recursion matrix\n",
    "    policy=np.empty_like(J)  #beginning of the optimal policy function\n",
    "    for x in range(n):        \n",
    "        policy[x] = np.argmax(R[x, :] + beta*Next[x,:])  #true Bellman recursion\n",
    "                              #arg.max finds the k that maximizes for row x\n",
    "        print('The optimal policy in state', state_names[int(x)], 'is to choose action', control_names[int(policy[x])])\n",
    "    return(policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The optimal policy in state In Jail is to choose action rob a bank\n",
      "The optimal policy in state Feel Sick is to choose action rob a bank\n",
      "The optimal policy in state Feel Normal is to choose action rob a bank\n",
      "The optimal policy in state Feel Great is to choose action rob a bank\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([4., 4., 4., 4.])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimal_policy(J,reward,transition,beta,state_names,control_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This answer made me laugh. I'm going to change some of the rewards."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "reward2 = np.array([[-50,-70,-60,-100,0],[-5,-15,-2,-30,30],[5,5,4,30,50],[10,15,12,20,30]])\n",
    "J = compute_value(reward2,transition,beta,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The optimal policy in state In Jail is to choose action rob a bank\n",
      "The optimal policy in state Feel Sick is to choose action Study\n",
      "The optimal policy in state Feel Normal is to choose action Watch TV\n",
      "The optimal policy in state Feel Great is to choose action Study\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([4., 2., 0., 2.])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "policy = optimal_policy(J,reward2,transition,beta,state_names,control_names)\n",
    "policy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So looking at these, they make a bit more sense. The first set of optimal policy choices was to always rob a bank. The reward was worth it. However, The change in rewards (maybe I suddenly gained a conscience) with a reduction in reward meant that it was no longer the optimal policy in all states. The only state where I would make the attempt to rob a bank was when I was already in jail. I never worked out. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ergodic(m):\n",
    "    \"\"\"\n",
    "    This code finds the ergodic distribution of a Markov matrix\n",
    "    It has a couple of important updates.\n",
    "    np.allclose() is a useful function \n",
    "    The calculation of the eigenvalues was subject to rounding error.\n",
    "    Now I use the left nullspace of m instead\n",
    "    \"\"\"\n",
    "    n=m.shape[0] # number of rows\n",
    "    T = 1000 # useful constant\n",
    "    if (m.shape[0] != m.shape[1]): #test for a square matrix\n",
    "        return('Matrix is not square') \n",
    "    if (not np.allclose(m@np.ones(n),np.ones(n))): #test row sums \n",
    "        return('This is not a Markov matrix')\n",
    "    if( not ((m >= 0).all() and (m <= 1).all())): #test probabilities\n",
    "        return('This is not a Markov matrix')\n",
    "    m_long = np.linalg.matrix_power(m,T)\n",
    "    if( not ((m_long > 0).all() and (m_long < 1).all())): #crude test for aperiodic and irreducible\n",
    "        return('This matrix may not be both aperiodic and irreducible')\n",
    "    m0=null_space(m.T-np.identity(n)) #need the nullspace of the rows.   That's why I transpose\n",
    "    return(m0/sum(m0)) # make it into a probability distribution\n",
    "def ergodic_states(policy,transition,state_names):\n",
    "    \"\"\"\n",
    "    policy is the policy function,\n",
    "    transition is the transition rule\n",
    "    state_names is a list of the states\n",
    "    returns the Markov matrix for the system\n",
    "    and also the ergodic distribution of the states\n",
    "    \"\"\"\n",
    "    n = policy.shape[0]  # number of states\n",
    "    M_optimal = np.zeros((n,n)) # zero out the array\n",
    "    for i in range(n):\n",
    "        M_optimal[i] = transition[int(policy[i])][i]  #pick the row corresponding to the optimal policy in state i\n",
    "    ergodic_dist = ergodic(M_optimal)    \n",
    "    print('The states are', state_names)\n",
    "    print('Their ergodic distribution is', ergodic_dist)\n",
    "    return(M_optimal,ergodic_dist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The states are ['In Jail', 'Feel Sick', 'Feel Normal', 'Feel Great']\n",
      "Their ergodic distribution is This matrix may not be both aperiodic and irreducible\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([[1. , 0. , 0. , 0. ],\n",
       "        [0. , 0.4, 0.6, 0. ],\n",
       "        [0. , 0.1, 0.7, 0.2],\n",
       "        [0. , 0.1, 0.8, 0.1]]),\n",
       " 'This matrix may not be both aperiodic and irreducible')"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ergodic_states(policy,transition,state_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is funny looking, but it looks like if I'm in jail I'm going to be in jail forever. I guess that's because the optimal policy is telling me I should try robbing a bank while I'm in jail. As difficult as that sounds, the transition rule that I made up arbitrarily has me going back to jail for sure if I try robbing a bank while I'm in jail. It's a silly scenario, but it all makes sense. The other states don't have me trying to do anything that puts me in jail (which makes sense since being in jail is forever here). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
